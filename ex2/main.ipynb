{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "# nltk.download(\"brown\")\n",
    "tagged_sents = list(nltk.corpus.brown.tagged_sents(categories=\"news\"))\n",
    "training_size = int(len(tagged_sents) * 0.9)\n",
    "training_data = tagged_sents[:training_size]\n",
    "test_data = tagged_sents[training_size:]\n",
    "\n",
    "tags_histogram = defaultdict(int)\n",
    "words_map = defaultdict(lambda: defaultdict(int))\n",
    "for sentence in training_data:\n",
    "    for word, part_of_speech in sentence:\n",
    "        words_map[word][part_of_speech] += 1\n",
    "        tags_histogram[part_of_speech] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Known words prediction err: 0.08315517047372567\nUnknown words prediction err: 0.7897033158813264\nTotal prediction err: 0.16385926442738963\n"
     ]
    }
   ],
   "source": [
    "# Baseline - most likely tag with no other assumptions\n",
    "most_probable_tag = dict()\n",
    "for word in words_map:\n",
    "    most_probable_tag[word] = max(words_map[word].items(), key=lambda pair: pair[1])[0]\n",
    "\n",
    "known_words_count = 0\n",
    "known_hits_count = 0\n",
    "unknown_words_count = 0\n",
    "unknown_hits_count = 0\n",
    "\n",
    "for sentence in test_data:\n",
    "    for word, real_tag in sentence:\n",
    "        if word in most_probable_tag:\n",
    "            predicted_tag = most_probable_tag[word]\n",
    "            known_words_count += 1\n",
    "            if predicted_tag == real_tag:\n",
    "                known_hits_count += 1\n",
    "        else:\n",
    "            predicted_tag = \"NN\"\n",
    "            unknown_words_count += 1\n",
    "            if predicted_tag == real_tag: \n",
    "                unknown_hits_count += 1\n",
    "known_words_accuracy = known_hits_count / known_words_count\n",
    "unknown_words_accuracy = unknown_hits_count / unknown_words_count\n",
    "total_accuracy = (known_hits_count + unknown_hits_count) / (known_words_count + unknown_words_count)\n",
    "\n",
    "print(\"Known words prediction err:\", 1 - known_words_accuracy)\n",
    "print(\"Unknown words prediction err:\", 1 - unknown_words_accuracy)\n",
    "print(\"Total prediction err:\", 1 - total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.7328530181074028e-69 CC NNS 37\ndefaultdict(<class 'float'>, {'AT': ('NN', 0), 'NP-TL': ('NN', 0), 'NN-TL': ('NN', 0), 'JJ-TL': ('NN', 0), 'VBD': ('NN', 0), 'NR': ('NN', 0), 'NN': ('JJ', 4.726809495838191e-15), 'IN': ('NN', 0), 'NP$': ('NN', 0), 'JJ': ('NN', 0), '``': ('NN', 0), \"''\": ('NN', 0), 'CS': ('NN', 0), 'DTI': ('NN', 0), 'NNS': ('NN', 0), '.': ('NN', 0), 'RBR': ('NN', 0), ',': ('NN', 0), 'WDT': ('NN', 0), 'HVD': ('NN', 0), 'VBZ': ('NN', 0), 'CC': ('NN', 0), 'IN-TL': ('NN', 0), 'BEDZ': ('NN', 0), 'VBN': ('NN', 0), 'NP': ('NN', 0), 'BEN': ('NN', 0), 'TO': ('NN', 0), 'VB': ('NN', 0), 'RB': ('NN', 0), 'DT': ('NN', 0), 'PPS': ('NN', 0), 'DOD': ('NN', 0), 'AP': ('NN', 0), 'BER': ('NN', 0), 'HV': ('NN', 0), 'DTS': ('NN', 0), 'VBG': ('NN', 0), 'PPO': ('NN', 0), 'QL': ('NN', 0), 'JJT': ('NN', 0), 'ABX': ('NN', 0), 'NN-HL': ('NN', 0), 'VBN-HL': ('NN', 0), 'WRB': ('NN', 0), 'CD': ('NN', 0), 'MD': ('NN', 0), 'BE': ('NN', 0), 'JJR': ('NN', 0), 'VBG-TL': ('NN', 0), 'BEZ': ('NN', 0), 'NN$-TL': ('NN', 0), 'HVZ': ('NN', 0), 'ABN': ('NN', 0), 'PN': ('NN', 0), 'PPSS': ('NN', 0), 'PP$': ('NN', 0), 'DO': ('NN', 0), 'NN$': ('NN', 0), 'NNS-HL': ('NN', 0), 'WPS': ('NN', 0), '*': ('NN', 0), 'EX': ('NN', 0), 'VB-HL': ('NN', 0), ':': ('NN', 0), '(': ('NN', 0), ')': ('NN', 0), 'NNS-TL': ('NN', 0), 'NPS': ('NN', 0), 'JJS': ('NN', 0), 'RP': ('NN', 0), '--': ('NN', 0), 'BED': ('NN', 0), 'OD': ('NN', 0), 'BEG': ('NN', 0), 'AT-HL': ('NN', 0), 'VBG-HL': ('NN', 0), 'AT-TL': ('NN', 0), 'PPL': ('NN', 0), 'DOZ': ('NN', 0), 'NP-HL': ('NN', 0), 'NR$': ('NN', 0), 'DOD*': ('NN', 0), 'BEDZ*': ('NN', 0), ',-HL': ('NN', 0), 'CC-TL': ('NN', 0), 'MD*': ('NN', 0), 'NNS$': ('NN', 0), 'PPSS+BER': ('NN', 0), \"'\": ('NN', 0), 'PPSS+BEM': ('NN', 0), 'CD-TL': ('NN', 0), 'RBT': ('NN', 0), '(-HL': ('NN', 0), ')-HL': ('NN', 0), 'MD-HL': ('NN', 0), 'VBZ-HL': ('NN', 0), 'IN-HL': ('NN', 0), 'JJ-HL': ('NN', 0), 'PPLS': ('NN', 0), 'CD-HL': ('NN', 0), 'WPO': ('NN', 0), 'JJS-TL': ('NN', 0), 'ABL': ('NN', 0), 'BER-HL': ('NN', 0), 'PPS+HVZ': ('NN', 0), 'VBD-HL': ('NN', 0), 'RP-HL': ('NN', 0), 'MD*-HL': ('NN', 0), 'AP-HL': ('NN', 0), 'CS-HL': ('NN', 0), 'DT$': ('NN', 0), 'HVN': ('NN', 0), 'FW-IN': ('NN', 0), 'FW-DT': ('NN', 0), 'VBN-TL': ('NN', 0), 'NR-TL': ('NN', 0), 'NNS$-TL': ('NN', 0), 'FW-NN': ('NN', 0), 'HVG': ('NN', 0), 'DTX': ('NN', 0), 'OD-TL': ('NN', 0), 'BEM': ('NN', 0), 'RB-HL': ('NN', 0), 'PPSS+MD': ('NN', 0), 'NPS-HL': ('NN', 0), 'NPS$': ('NN', 0), 'WP$': ('NN', 0), 'NN-TL-HL': ('NN', 0), 'CC-HL': ('NN', 0), 'PPS+BEZ': ('NN', 0), 'AP-TL': ('NN', 0), 'UH-TL': ('NN', 0), 'BEZ-HL': ('NN', 0), 'TO-HL': ('NN', 0), 'DO*': ('NN', 0), 'VBN-TL-HL': ('NN', 0), 'NNS-TL-HL': ('NN', 0), 'DT-HL': ('NN', 0), 'BE-HL': ('NN', 0), 'DOZ*': ('NN', 0), 'QLP': ('NN', 0), 'JJR-HL': ('NN', 0), 'PPSS+HVD': ('NN', 0), 'FW-IN+NN': ('NN', 0), 'PP$$': ('NN', 0), 'JJT-HL': ('NN', 0), 'NP-TL-HL': ('NN', 0), 'NPS-TL': ('NN', 0), 'MD+HV': ('NN', 0), 'NP$-TL': ('NN', 0), 'OD-HL': ('NN', 0), 'JJR-TL': ('NN', 0), 'VBD-TL': ('NN', 0), 'DT+BEZ': ('NN', 0), 'EX+BEZ': ('NN', 0), 'PPSS+HV': ('NN', 0), ':-HL': ('NN', 0), 'PPS+MD': ('NN', 0), 'UH': ('NN', 0), 'FW-CC': ('NN', 0), 'FW-NNS': ('NN', 0), 'BEDZ-HL': ('NN', 0), 'NN$-HL': ('NN', 0), '.-HL': ('NN', 0), 'HVD*': ('NN', 0), 'BEZ*': ('NN', 0), 'AP$': ('NN', 0), 'NP+BEZ': ('NN', 0), 'FW-AT-TL': ('NN', 0), 'VB-TL': ('NN', 0), 'RB-TL': ('NN', 0), 'MD-TL': ('NN', 0), 'PN+HVZ': ('NN', 0), 'FW-JJ-TL': ('NN', 0), 'FW-NN-TL': ('NN', 0), 'ABN-HL': ('NN', 0), 'PPS+BEZ-HL': ('NN', 0), 'NR-HL': ('NN', 0), 'HVD-HL': ('NN', 0), 'RB$': ('NN', 0), 'FW-AT-HL': ('NN', 0), 'DO-HL': ('NN', 0), 'PP$-TL': ('NN', 0), 'FW-IN-TL': ('NN', 0), 'WPS+BEZ': ('NN', 0), '*-HL': ('NN', 0), 'DTI-HL': ('NN', 0), 'PN-HL': ('NN', 0), 'CD$': ('NN', 0), 'BER*': ('NN', 0), 'NNS$-HL': ('NN', 0), 'PN$': ('NN', 0), 'BER-TL': ('NN', 0), 'TO-TL': ('NN', 0), 'FW-JJ': ('NN', 0), 'BED*': ('NN', 0), 'RB+BEZ': ('NN', 0), 'VB+PPO': ('NN', 0), 'PPSS-HL': ('NN', 0), 'HVZ*': ('NN', 0), 'FW-IN+NN-TL': ('NN', 0), 'FW-IN+AT-TL': ('NN', 0), 'NN-NC': ('NN', 0), 'JJ-NC': ('NN', 0), 'NR$-TL': ('NN', 0), 'FW-PP$-NC': ('NN', 0), 'FW-VB': ('NN', 0), 'FW-VB-NC': ('NN', 0), 'JJR-NC': ('NN', 0), 'NPS$-TL': ('NN', 0), 'QL-TL': ('NN', 0)})\n"
     ]
    }
   ],
   "source": [
    "# Bigram HMM\n",
    "emissions = defaultdict(lambda: defaultdict(float))\n",
    "for word in words_map:\n",
    "    for part_of_speech in words_map[word]:\n",
    "        emissions[word][part_of_speech] = words_map[word][part_of_speech] / tags_histogram[part_of_speech]\n",
    "\n",
    "transitions = defaultdict(float)\n",
    "for sentence in training_data:\n",
    "    previous_tag = '*'\n",
    "    for word, tag in sentence:\n",
    "        transitions[(previous_tag, tag)] += 1\n",
    "        previous_tag = tag\n",
    "    transitions[(previous_tag, \"STOP\")] += 1\n",
    "for transition in transitions:\n",
    "    transitions[transition] /= tags_histogram[transition[0]]\n",
    "\n",
    "def viterbi(sentence, transitions, emissions):\n",
    "    n = len(sentence)\n",
    "    # Calling pi pie so I won't confuse it with pi\n",
    "    pie = [defaultdict(float) for i in range(n + 1)]\n",
    "    for tag in tags_histogram.keys():\n",
    "        pie[0][tag] = (None, 1.0)\n",
    "    for k in range(1, n + 1):\n",
    "        for tag in tags_histogram.keys():\n",
    "            max_arg = \"NN\"\n",
    "            max_value = 0\n",
    "            for previous_tag in tags_histogram.keys():\n",
    "                emission = emissions[sentence[k - 1][0]][tag]\n",
    "                transition = transitions[(previous_tag, tag)]\n",
    "                pr = pie[k - 1][previous_tag][1] * emission * transition\n",
    "                if pr > max_value:\n",
    "                    max_arg = previous_tag\n",
    "                    max_value = pr\n",
    "            pie[k][tag] = (max_arg, max_value)\n",
    "\n",
    "viterbi(training_data[3], transitions, emissions)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}