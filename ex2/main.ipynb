{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import re\n",
    "\n",
    "# Use !* because apparently, the word \"not\" is tagged with * \n",
    "START_MARKER = '!*'\n",
    "STOP_MARKER = 'STOP'\n",
    "\n",
    "\n",
    "def prep_part_of_speech(target):\n",
    "    if '+' in target or '-' in target and len(target) > 2 and target != '--':\n",
    "        matches = re.search(r'(.+?)([\\-\\+])', target)\n",
    "        return matches.group(1)\n",
    "    return target\n",
    "\n",
    "\n",
    "def prep_dataset(dataset):\n",
    "    return list(map(lambda sentence: list(map(lambda pair: (pair[0], prep_part_of_speech(pair[1])), sentence)), dataset))\n",
    "\n",
    "def transform_pseudo(dataset):\n",
    "    words_histogram = defaultdict(int)\n",
    "    for sentence in dataset:\n",
    "        for word, tag in sentence:\n",
    "            words_histogram[word] += 1\n",
    "    low_frequency_words = [word for word, count in words_histogram.items() if count <= 4]\n",
    "    for sentence in dataset:\n",
    "        for i in range(len(sentence)):\n",
    "            word = sentence[i][0]\n",
    "            tag = sentence[i][1]\n",
    "            if word in low_frequency_words:\n",
    "                if word.isdigit():\n",
    "                    if len(word) == 2:\n",
    "                        pseudo = \"2Digits\"\n",
    "                    else:\n",
    "                        pseudo = \"allDigits\"\n",
    "                elif word.isupper():\n",
    "                    pseudo = \"allCaps\"\n",
    "                elif word[0].isupper() and word[-1] == \".\":\n",
    "                    pseudo = \"capPeriod\"\n",
    "                elif word[0].isupper():\n",
    "                    pseudo = \"initCaps\"\n",
    "                else:\n",
    "                    pseudo = word\n",
    "                sentence[i] = (pseudo, tag)\n",
    "    return dataset\n",
    "\n",
    "def create_words_map_tags_histogram(dataset):\n",
    "    words_map = defaultdict(lambda: defaultdict(int))\n",
    "    tags_histogram = defaultdict(int)\n",
    "    for sentence in dataset:\n",
    "        for word, part_of_speech in sentence:\n",
    "            words_map[word][part_of_speech] += 1\n",
    "            tags_histogram[part_of_speech] += 1\n",
    "    tags_histogram[START_MARKER] = len(training_data)\n",
    "    tags_histogram[STOP_MARKER] = len(training_data)\n",
    "    return words_map, tags_histogram\n",
    "\n",
    "\n",
    "\n",
    "# nltk.download(\"brown\")\n",
    "tagged_sents = list(nltk.corpus.brown.tagged_sents(categories=\"news\"))[:1000]\n",
    "training_size = int(len(tagged_sents) * 0.9)\n",
    "training_data = prep_dataset(tagged_sents[:training_size])\n",
    "test_data = prep_dataset(tagged_sents[training_size:])\n",
    "words_map, tags_histogram = create_words_map_tags_histogram(training_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Known words prediction err: 0.05176211453744495\nUnknown words prediction err: 0.7465940054495912\nTotal prediction err: 0.16857535501603294\n"
     ]
    }
   ],
   "source": [
    "# Baseline - most likely tag with no other assumptions\n",
    "most_probable_tag = dict()\n",
    "for word in words_map:\n",
    "    most_probable_tag[word] = max(words_map[word].items(), key=lambda pair: pair[1])[0]\n",
    "\n",
    "known_words_count = 0\n",
    "known_hits_count = 0\n",
    "unknown_words_count = 0\n",
    "unknown_hits_count = 0\n",
    "\n",
    "for sentence in test_data:\n",
    "    for word, real_tag in sentence:\n",
    "        if word in most_probable_tag:\n",
    "            predicted_tag = most_probable_tag[word]\n",
    "            known_words_count += 1\n",
    "            if predicted_tag == real_tag:\n",
    "                known_hits_count += 1\n",
    "        else:\n",
    "            predicted_tag = \"NN\"\n",
    "            unknown_words_count += 1\n",
    "            if predicted_tag == real_tag: \n",
    "                unknown_hits_count += 1\n",
    "known_words_accuracy = known_hits_count / known_words_count\n",
    "unknown_words_accuracy = unknown_hits_count / unknown_words_count\n",
    "total_accuracy = (known_hits_count + unknown_hits_count) / (known_words_count + unknown_words_count)\n",
    "\n",
    "print(\"Known words prediction err:\", 1 - known_words_accuracy)\n",
    "print(\"Unknown words prediction err:\", 1 - unknown_words_accuracy)\n",
    "print(\"Total prediction err:\", 1 - total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Baseline Viterbi:\n",
      "Viterbi Known words prediction err: 0.7940528634361234\n",
      "Viterbi Unknown words prediction err: 0.7493188010899183\n",
      "Viterbi Total prediction err: 0.7865322950068713\n"
     ]
    }
   ],
   "source": [
    "# Bigram HMM\n",
    "def viterbi(sentence, transitions, emissions):\n",
    "    n = len(sentence)\n",
    "    \n",
    "    # Calling pi pie so I won't confuse it with PI\n",
    "    pie = [defaultdict(float) for i in range(n + 1)]\n",
    "    tags = set(tags_histogram.keys())\n",
    "    for tag in tags:\n",
    "        pie[0][tag] = (None, 1.0)\n",
    "    for k in range(1, n + 1):\n",
    "        for tag in tags:\n",
    "            max_arg = \"NN\"\n",
    "            max_value = 0\n",
    "            for previous_tag in tags:\n",
    "                emission = emissions[sentence[k - 1]][tag]\n",
    "                transition = transitions[(previous_tag, tag)]\n",
    "\n",
    "                pr = pie[k - 1][previous_tag][1] * emission * transition\n",
    "                if pr > max_value:\n",
    "                    max_arg = previous_tag\n",
    "                    max_value = pr\n",
    "            \n",
    "            pie[k][tag] = (max_arg, max_value)\n",
    "\n",
    "    predictions = [None for i in range(n)]\n",
    "    # Pair shape (I miss typescript!): (<current_tag>, (<previous_tag>, <previous_value>))\n",
    "    last_step_max = max(pie[n].items(), key=lambda pair: pair[1][1] * transitions[(pair[1], STOP_MARKER)] )\n",
    "    predictions[n - 1] = last_step_max[0]\n",
    "\n",
    "    for k in range(n - 2, -1, -1):\n",
    "        predictions[k] = pie[k + 2][predictions[k + 1]][0]\n",
    "    \n",
    "    return predictions\n",
    "    \n",
    "\n",
    "def print_viterbi_error_rate(dataset, transitions, emissions):\n",
    "    known_words_count = 0\n",
    "    known_hits_count = 0\n",
    "    unknown_words_count = 0\n",
    "    unknown_hits_count = 0\n",
    "    for sentence in dataset:\n",
    "        predictions = viterbi([pair[0] for pair in sentence], transitions, emissions)\n",
    "        for i in range(len(sentence)):\n",
    "            if sentence[i][0] in words_map:\n",
    "                known_words_count += 1\n",
    "                if sentence[i][1] == predictions[i]:\n",
    "                    known_hits_count += 1\n",
    "            else:\n",
    "                unknown_words_count += 1\n",
    "                if sentence[i][1] == predictions[i]:\n",
    "                    unknown_hits_count += 1\n",
    "            \n",
    "    known_words_accuracy = known_hits_count / known_words_count\n",
    "    unknown_words_accuracy = unknown_hits_count / unknown_words_count\n",
    "    total_accuracy = (known_hits_count + unknown_hits_count) / (known_words_count + unknown_words_count)\n",
    "    print(\"Viterbi Known words prediction err:\", 1 - known_words_accuracy)\n",
    "    print(\"Viterbi Unknown words prediction err:\", 1 - unknown_words_accuracy)\n",
    "    print(\"Viterbi Total prediction err:\", 1 - total_accuracy)\n",
    "\n",
    "\n",
    "def calculate_emissions(words_map, tags_histogram, smoothing_count=0):\n",
    "    emissions = defaultdict(lambda: defaultdict(float))\n",
    "    for word in words_map:\n",
    "        for part_of_speech in words_map[word]:\n",
    "            emissions[word][part_of_speech] = (words_map[word][part_of_speech] + smoothing_count) / (tags_histogram[part_of_speech] + smoothing_count * len(tags_histogram))\n",
    "    return emissions\n",
    "\n",
    "def calculate_transitions(dataset):\n",
    "    transitions = defaultdict(float)\n",
    "    for sentence in dataset:\n",
    "        previous_tag = START_MARKER\n",
    "        for word, tag in sentence:\n",
    "            transitions[(previous_tag, tag)] += 1\n",
    "            previous_tag = tag\n",
    "        transitions[(previous_tag, STOP_MARKER)] += 1\n",
    "    for transition in transitions:\n",
    "        transitions[transition] /= len(training_data)\n",
    "    return transitions\n",
    "\n",
    "\n",
    "print(\"Baseline Viterbi:\")\n",
    "print_viterbi_error_rate(test_data, calculate_transitions(training_data), calculate_emissions(words_map, tags_histogram, 0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Add-one Viterbi:\n",
      "Viterbi Known words prediction err: 0.7940528634361234\n",
      "Viterbi Unknown words prediction err: 0.7493188010899183\n",
      "Viterbi Total prediction err: 0.7865322950068713\n"
     ]
    }
   ],
   "source": [
    "# Add one smoothing\n",
    "print(\"Add-one Viterbi:\")\n",
    "print_viterbi_error_rate(test_data, calculate_transitions(training_data), calculate_emissions(words_map, tags_histogram, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Viterbi w/ pseudo words\n",
      "Viterbi Known words prediction err: 0.7764920828258222\n",
      "Viterbi Unknown words prediction err: 0.7208872458410351\n",
      "Viterbi Total prediction err: 0.7627118644067796\n"
     ]
    }
   ],
   "source": [
    "# Pseudo words\n",
    "transformed_training_data = transform_pseudo(training_data)\n",
    "transformed_test_data = transform_pseudo(test_data)\n",
    "transformed_words_map, transformed_tags_histogram = create_words_map_tags_histogram(transformed_training_data)\n",
    "print(\"Viterbi w/ pseudo words\")\n",
    "print_viterbi_error_rate(transformed_test_data, calculate_transitions(transformed_training_data), calculate_emissions(transformed_words_map, transformed_tags_histogram))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Add-one Viterbi w/ pseudo words\n",
      "Viterbi Known words prediction err: 0.7764920828258222\n",
      "Viterbi Unknown words prediction err: 0.7208872458410351\n",
      "Viterbi Total prediction err: 0.7627118644067796\n"
     ]
    }
   ],
   "source": [
    "# Pseudo words and add one\n",
    "print(\"Add-one Viterbi w/ pseudo words\")\n",
    "print_viterbi_error_rate(transformed_test_data, calculate_transitions(transformed_training_data), calculate_emissions(transformed_words_map, transformed_tags_histogram, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "# got me confused"
   ]
  }
 ]
}