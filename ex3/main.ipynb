{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Question 4\n",
    "----\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "stop_words = ['is', 'a', 'of', 'and']\n",
    "\n",
    "def co_occurrence(sentences, window_size):\n",
    "    histogram = defaultdict(int)\n",
    "    vocab = set()\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split(' ')\n",
    "        words = list(filter(lambda x: x not in stop_words, words))\n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            vocab.add(word)\n",
    "            rest_window = words[i + 1 : i + 1 + window_size]\n",
    "            for neighbor_word in rest_window:\n",
    "                key = tuple(sorted([neighbor_word, word]))\n",
    "                histogram[key] += 1\n",
    "\n",
    "    vocab = sorted(vocab)\n",
    "    df = pd.DataFrame(data=np.zeros((len(vocab), len(vocab)), dtype=np.int16),\n",
    "                      index=vocab,\n",
    "                      columns=vocab)\n",
    "    for key, value in histogram.items():\n",
    "        df.at[key[0], key[1]] = value\n",
    "        df.at[key[1], key[0]] = value\n",
    "    return df"
   ]
  },
  {
   "source": [
    "**1) Co-occurence matrix.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "          Deep  He  John  Mary  NLP  about  got  learning  likes  machine  \\\nDeep         0   0     0     0    0      0    0         1      0        0   \nHe           0   0     0     0    0      0    0         0      1        0   \nJohn         0   0     0     0    0      0    0         0      2        0   \nMary         0   0     0     0    0      0    0         0      1        0   \nNLP          0   0     0     0    0      1    1         0      1        0   \nabout        0   0     0     0    1      0    0         0      0        0   \ngot          0   0     0     0    1      0    0         0      1        0   \nlearning     1   0     0     0    0      0    0         0      0        2   \nlikes        0   1     2     1    1      0    1         0      0        1   \nmachine      0   0     0     0    0      0    0         2      1        0   \npost         0   0     0     0    0      1    0         0      0        0   \nsubfield     0   0     0     0    0      0    0         1      0        1   \nwrote        0   0     1     0    0      0    0         0      0        0   \n\n          post  subfield  wrote  \nDeep         0         0      0  \nHe           0         0      0  \nJohn         0         0      1  \nMary         0         0      0  \nNLP          0         0      0  \nabout        1         0      0  \ngot          0         0      0  \nlearning     0         1      0  \nlikes        0         0      0  \nmachine      0         1      0  \npost         0         0      1  \nsubfield     0         0      0  \nwrote        1         0      0  \n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'John likes NLP',\n",
    "    'He likes Mary',\n",
    "    'John likes machine learning',\n",
    "    'Deep learning is a subfield of machine learning',\n",
    "    'John wrote a post about NLP and got likes'\n",
    "]\n",
    "\n",
    "co_occurrence_df = co_occurrence(sentences, 1)\n",
    "np.set_printoptions(linewidth=300)\n",
    "print(co_occurrence_df)"
   ]
  },
  {
   "source": [
    "**2) Singular Value Decomposition and eigenvalues.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-0.09525618 -0.078084   -0.20409309 -0.30660432  0.10840231  0.06262278 -0.00986875  0.11843932 -0.22723703  0.01384834  0.69775862 -0.52978814  0.        ]\n [-0.16408222 -0.20187228  0.11351611  0.08518163 -0.11456372 -0.16601357  0.0282392  -0.14589432 -0.11169003 -0.22682727 -0.28939733 -0.44915337 -0.70710678]\n [-0.36361678 -0.45759106  0.28167501  0.26909965 -0.1014125  -0.07594124  0.45395368  0.19533118 -0.1045044   0.41073465  0.18797743  0.17719595  0.        ]\n [-0.16408222 -0.20187228  0.11351611  0.08518163 -0.11456372 -0.16601357  0.0282392  -0.14589432 -0.11169003 -0.22682727 -0.28939733 -0.44915337  0.70710678]\n [-0.25987227 -0.18258385  0.23872932  0.1509113   0.43876455  0.17183454 -0.52683758 -0.50615785 -0.14151891  0.12692599  0.11614814  0.11806938 -0.        ]\n [-0.09294414  0.08314097  0.1288352  -0.15246158 -0.53761061  0.54461547 -0.29465061  0.09041336 -0.04576066  0.43260971 -0.19017074 -0.19639523  0.        ]\n [-0.2390606  -0.14409564  0.2034698   0.01369787 -0.37933497 -0.04890169 -0.39549232  0.3097711   0.06910896 -0.56527546  0.27380553  0.28732963 -0.        ]\n [-0.33015435  0.24675851 -0.54164539  0.64728074 -0.17963842  0.09188441 -0.0122701  -0.13156362  0.17786789 -0.00519345  0.14389729 -0.08493307 -0.        ]\n [-0.56870279  0.6379502   0.30126193 -0.17982925  0.18984878 -0.24358643  0.03511061  0.16206091  0.08742444  0.08506553 -0.0596818  -0.07200609  0.        ]\n [-0.4167716  -0.37042327 -0.43305827 -0.4935293   0.05791577  0.00357016  0.00159817 -0.08250592  0.43636806  0.03863315 -0.19942192  0.12697215 -0.        ]\n [-0.06226861 -0.08015554  0.10318815  0.17095449  0.45213457  0.6272624   0.16049     0.40572576  0.17733768 -0.28916475 -0.15536665 -0.14955451  0.        ]\n [-0.21550319  0.03913235 -0.36727033 -0.07282907  0.0734532   0.06505598 -0.00858336  0.19271476 -0.7847237  -0.08916697 -0.26923915  0.26222778 -0.        ]\n [-0.12287652  0.17016396  0.14501723 -0.20844511 -0.21164226  0.37574669  0.49419245 -0.54109773 -0.09304894 -0.32416617  0.1581298   0.17241938 -0.        ]] \n\n [3.46596239 3.16016742 2.65391343 2.11112724 1.65714573 1.46726817 1.24332876 1.11081033 0.78274168 0.37502339 0.2062279  0.16031515 0.        ] \n\n [[-0.09525618 -0.16408222 -0.36361678 -0.16408222 -0.25987227 -0.09294414 -0.2390606  -0.33015435 -0.56870279 -0.4167716  -0.06226861 -0.21550319 -0.12287652]\n [ 0.078084    0.20187228  0.45759106  0.20187228  0.18258385 -0.08314097  0.14409564 -0.24675851 -0.6379502   0.37042327  0.08015554 -0.03913235 -0.17016396]\n [-0.20409309  0.11351611  0.28167501  0.11351611  0.23872932  0.1288352   0.2034698  -0.54164539  0.30126193 -0.43305827  0.10318815 -0.36727033  0.14501723]\n [ 0.30660432 -0.08518163 -0.26909965 -0.08518163 -0.1509113   0.15246158 -0.01369787 -0.64728074  0.17982925  0.4935293  -0.17095449  0.07282907  0.20844511]\n [-0.10840231  0.11456372  0.1014125   0.11456372 -0.43876455  0.53761061  0.37933497  0.17963842 -0.18984878 -0.05791577 -0.45213457 -0.0734532   0.21164226]\n [ 0.06262278 -0.16601357 -0.07594124 -0.16601357  0.17183454  0.54461547 -0.04890169  0.09188441 -0.24358643  0.00357016  0.6272624   0.06505598  0.37574669]\n [-0.00986875  0.0282392   0.45395368  0.0282392  -0.52683758 -0.29465061 -0.39549232 -0.0122701   0.03511061  0.00159817  0.16049    -0.00858336  0.49419245]\n [-0.11843932  0.14589432 -0.19533118  0.14589432  0.50615785 -0.09041336 -0.3097711   0.13156362 -0.16206091  0.08250592 -0.40572576 -0.19271476  0.54109773]\n [ 0.22723703  0.11169003  0.1045044   0.11169003  0.14151891  0.04576066 -0.06910896 -0.17786789 -0.08742444 -0.43636806 -0.17733768  0.7847237   0.09304894]\n [-0.01384834  0.22682727 -0.41073465  0.22682727 -0.12692599 -0.43260971  0.56527546  0.00519345 -0.08506553 -0.03863315  0.28916475  0.08916697  0.32416617]\n [ 0.69775862 -0.28939733  0.18797743 -0.28939733  0.11614814 -0.19017074  0.27380553  0.14389729 -0.0596818  -0.19942192 -0.15536665 -0.26923915  0.1581298 ]\n [-0.52978814 -0.44915337  0.17719595 -0.44915337  0.11806938 -0.19639523  0.28732963 -0.08493307 -0.07200609  0.12697215 -0.14955451  0.26222778  0.17241938]\n [-0.         -0.70710678  0.          0.70710678  0.          0.          0.         -0.         -0.          0.         -0.          0.          0.        ]] \n\n\neigenvalues: [12.01289529  9.98665815  7.0432565   4.45685823  2.74613196  2.15287589  1.54586641  1.2338996   0.61268454  0.14064255  0.04252995  0.02570095  0.        ]\n"
     ]
    }
   ],
   "source": [
    "co_occurrence_matrix = co_occurrence_df.to_numpy()\n",
    "u, s, v = np.linalg.svd(co_occurrence_matrix)\n",
    "print(u, \"\\n\\n\", s, \"\\n\\n\", v, \"\\n\\n\")\n",
    "print(\"eigenvalues:\", s**2)"
   ]
  },
  {
   "source": [
    "**3) Reduced matrix.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.12272743 -0.05712672 -0.14543279 -0.05712672 -0.08856279 -0.01858134 -0.06683841  0.46327138  0.1820022   0.28075788 -0.0551122   0.27973583  0.00400971]\n",
      " [-0.05712672 -0.00127236 -0.00027247 -0.00127236  0.10323074  0.14471053  0.1053263   0.1820022   0.82116207 -0.1297564   0.01536375  0.03687718  0.22212452]\n",
      " [-0.14543279 -0.00027247  0.00711724 -0.00027247  0.24194435  0.33367263  0.24501427  0.36801412  1.86444865 -0.33413496  0.03970333  0.05363304  0.50933345]\n",
      " [-0.05712672 -0.00127236 -0.00027247 -0.00127236  0.10323074  0.14471053  0.1053263   0.1820022   0.82116207 -0.1297564   0.01536375  0.03687718  0.22212452]\n",
      " [-0.08856279  0.10323074  0.24194435  0.10323074  0.27996985  0.21331318  0.26109289  0.09658246  1.07119889 -0.11271466  0.07521302 -0.0160058   0.30073777]\n",
      " [-0.01858134  0.14471053  0.33367263  0.14471053  0.21331318  0.0521477   0.18444068 -0.14367498  0.11859449  0.08351377  0.07640111 -0.06643536  0.0444587 ]\n",
      " [-0.06683841  0.1053263   0.24501427  0.1053263   0.26109289  0.18444068  0.24233526  0.09343942  0.92439281 -0.05719942  0.07081478 -0.00194264  0.25760739]\n",
      " [ 0.46327138  0.1820022   0.36801412  0.1820022   0.09658246 -0.14367498  0.09343942  0.96397902 -0.27976235  1.3882808  -0.01457163  0.74402926 -0.20054499]\n",
      " [ 0.1820022   0.82116207  1.86444865  0.82116207  1.07119889  0.11859449  0.92439281 -0.27976235  0.07571096  1.22204366  0.36683505  0.0522458   0.01509128]\n",
      " [ 0.28075788 -0.1297564  -0.33413496 -0.1297564  -0.11271466  0.08351377 -0.05719942  1.3882808   1.22204366  0.66612889 -0.12247618  0.77920931  0.21002261]\n",
      " [-0.0551122   0.01536375  0.03970333  0.01536375  0.07521302  0.07640111  0.07081478 -0.01457163  0.36683505 -0.12247618  0.02139338 -0.04415541  0.10933598]\n",
      " [ 0.27973583  0.03687718  0.05363304  0.03687718 -0.0160058  -0.06643536 -0.00194264  0.74402926  0.0522458   0.77920931 -0.04415541  0.51410537 -0.07061245]\n",
      " [ 0.00400971  0.22212452  0.50933345  0.22212452  0.30073777  0.0444587   0.25760739 -0.20054499  0.01509128  0.21002261  0.10933598 -0.07061245  0.01663801]]\n"
     ]
    }
   ],
   "source": [
    "clipped_size = int(0.3 * s.shape[0])\n",
    "u_tag = u[:, :clipped_size]\n",
    "s_tag = np.diag(s[:clipped_size])\n",
    "v_tag = v[:clipped_size, :]\n",
    "x_tag = np.matmul(np.matmul(u_tag, s_tag), v_tag)\n",
    "print(x_tag)"
   ]
  },
  {
   "source": [
    "One practical advantage is that we need much less numbers to express the co-occurence matrix (it's like [JPEG compression](https://en.wikipedia.org/wiki/JPEG#JPEG_compression) in a way - we take x% of the crucial frequencies).\n",
    "The real advantage, however, is the reduced dimension, which means it's easier to work with our data (e.g. visualize, compute) and our data gets much more \"smooth\", it's continous rather than discrete.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "\n",
    "**4) Cosine similarity.**\n",
    "\n",
    "Now we're in the latent space (looking at U'), every word is described by only 3 features."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "John-he: 0.999241482928557\nJohn-subfield: -0.15497560206457914\nDeep-machine: 0.9329156098788491\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "john_vector = u_tag[2]\n",
    "he_vector = u_tag[1]\n",
    "subfield_vector = u_tag[11]\n",
    "deep_vector = u_tag[0]\n",
    "machine_vector = u_tag[9]\n",
    "\n",
    "print(\"John-he:\", cosine_similarity(john_vector, he_vector))\n",
    "print(\"John-subfield:\", cosine_similarity(john_vector, subfield_vector))\n",
    "print(\"Deep-machine:\", cosine_similarity(deep_vector, machine_vector))"
   ]
  },
  {
   "source": [
    "As we can see, our toy model captures the semantic similarity to some extent. Since our dataset is so small, it might not make any sense, but we used the words `John` and `He` interchangeably and our model learned it! This is exciting.\n",
    "\n",
    "On the other hand, our model knows that the words `John` and `Subfield` are not related because they are really far away from each other in our dataset - there are only few other words connecting them.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**6) Cosine similarity - special case.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "wrote-post: 0.243076522917071\nlikes-likes: 1.0\n"
     ]
    }
   ],
   "source": [
    "wrote_vector = u_tag[12]\n",
    "post_vector = u_tag[10]\n",
    "likes_vector = u_tag[8]\n",
    "print(\"wrote-post:\", cosine_similarity(wrote_vector, post_vector))\n",
    "print(\"likes-likes:\", cosine_similarity(likes_vector, likes_vector))"
   ]
  },
  {
   "source": [
    "It might be a problem since these words are semantically-related. Maybe we can add more examples with these words so the smoothing introduced with the dimension reduction process won't butch it.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**7)** We would expect these two words to have a similarity of 1, because they are the same, but our model turns out to outsmart us - we used these 2 words with 2 different semantic meanings! So we're losing data here (it feels like quantization). Maybe, we can use a POS tagged corpus (where milenial `likes` is a noun), and define an entity also by it's tag. This way we will be able to differentiate between the two meanings."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![](https://imgs.xkcd.com/comics/python.png)\n",
    "\n",
    "*Created with Jupyter using vscode. Not everything in 2020 sucks.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}